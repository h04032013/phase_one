The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.
Traceback (most recent call last):
  File "/Users/haylindiaz/Projects/Phase_One_Testing/phase_one_gen.py", line 58, in <module>
    responses = answer_questions(
  File "/Users/haylindiaz/Projects/Phase_One_Testing/phase_one_gen.py", line 14, in answer_questions
    outputs = model.generate(**inputs, max_new_tokens=2000)
  File "/opt/anaconda3/envs/genv/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/opt/anaconda3/envs/genv/lib/python3.10/site-packages/transformers/generation/utils.py", line 2223, in generate
    result = self._sample(
  File "/opt/anaconda3/envs/genv/lib/python3.10/site-packages/transformers/generation/utils.py", line 3204, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
  File "/Users/haylindiaz/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3.5-mini-instruct/3145e03a9fd4cdd7cd953c34d9bbf7ad606122ca/modeling_phi3.py", line 1299, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
  File "/opt/anaconda3/envs/genv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1931, in __getattr__
    raise AttributeError(
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'. Did you mean: 'get_seq_length'?

https://github.com/huggingface/transformers/issues/36071




model.safetensors.index.json: 100%|████████████████████████████████████████████████████████████████| 16.3k/16.3k [00:00<00:00, 92.0MB/s]
model-00001-of-00002.safetensors: 100%|████████████████████████████████████████████████████████████| 4.90G/4.90G [01:13<00:00, 66.6MB/s]
model-00002-of-00002.safetensors: 100%|████████████████████████████████████████████████████████████| 2.77G/2.77G [00:38<00:00, 72.5MB/s]
Downloading shards: 100%|█████████████████████████████████████████████████████████████████████████████████| 2/2 [01:52<00:00, 56.08s/it]
Loading checkpoint shards: 100%|██████████████████████████████████████████████████████████████████████████| 2/2 [00:17<00:00,  8.78s/it]
generation_config.json: 100%|██████████████████████████████████████████████████████████████████████████| 168/168 [00:00<00:00, 1.52MB/s]
tokenizer_config.json: 100%|███████████████████████████████████████████████████████████████████████| 2.93k/2.93k [00:00<00:00, 46.6MB/s]
vocab.json: 100%|██████████████████████████████████████████████████████████████████████████████████| 3.91M/3.91M [00:00<00:00, 36.2MB/s]
merges.txt: 100%|██████████████████████████████████████████████████████████████████████████████████| 2.42M/2.42M [00:00<00:00, 36.9MB/s]
tokenizer.json: 100%|██████████████████████████████████████████████████████████████████████████████| 15.5M/15.5M [00:00<00:00, 68.0MB/s]
added_tokens.json: 100%|███████████████████████████████████████████████████████████████████████████████| 249/249 [00:00<00:00, 5.07MB/s]
special_tokens_map.json: 100%|█████████████████████████████████████████████████████████████████████████| 587/587 [00:00<00:00, 11.8MB/s]
