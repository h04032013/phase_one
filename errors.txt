The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.
Traceback (most recent call last):
  File "/Users/haylindiaz/Projects/Phase_One_Testing/phase_one_gen.py", line 58, in <module>
    responses = answer_questions(
  File "/Users/haylindiaz/Projects/Phase_One_Testing/phase_one_gen.py", line 14, in answer_questions
    outputs = model.generate(**inputs, max_new_tokens=2000)
  File "/opt/anaconda3/envs/genv/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 116, in decorate_context
    return func(*args, **kwargs)
  File "/opt/anaconda3/envs/genv/lib/python3.10/site-packages/transformers/generation/utils.py", line 2223, in generate
    result = self._sample(
  File "/opt/anaconda3/envs/genv/lib/python3.10/site-packages/transformers/generation/utils.py", line 3204, in _sample
    model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)
  File "/Users/haylindiaz/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3.5-mini-instruct/3145e03a9fd4cdd7cd953c34d9bbf7ad606122ca/modeling_phi3.py", line 1299, in prepare_inputs_for_generation
    max_cache_length = past_key_values.get_max_length()
  File "/opt/anaconda3/envs/genv/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1931, in __getattr__
    raise AttributeError(
AttributeError: 'DynamicCache' object has no attribute 'get_max_length'. Did you mean: 'get_seq_length'?